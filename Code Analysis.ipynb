{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import scipy.io\n",
    "import nibabel as nib\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import random\n",
    "import pandas as pd\n",
    "from math import isnan\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.cluster import SpectralClustering\n",
    "import itertools\n",
    "from itertools import combinations \n",
    "from scipy.spatial import distance\n",
    "from sklearn.neighbors import radius_neighbors_graph as rng\n",
    "import pickle\n",
    "import _pickle as cPickle\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import f1_score, jaccard_similarity_score, adjusted_mutual_info_score, adjusted_rand_score, fowlkes_mallows_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unpacking data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_data(all_directories, sides):\n",
    "    #Function which takes list of directories to data and sides and returns dictionary with lists of files with rqas\n",
    "    mask_directories = os.listdir(all_directories)[:] #directories to all possible masks\n",
    "    dict_rqa_files={mask_name:[os.listdir(all_directories+mask_name+'/'+ hemisphere+'/') for hemisphere in sides] for mask_name in mask_directories if mask_name[0]!='.'} # dictionary with keys: names of masks and values: list of directories containing mask name and side\n",
    "    dict_rqa_files = {key:[[all_directories+key+'/'+sides[num] +'/'+one_file for one_file in value_side if one_file[0]!='.'] for num, value_side in enumerate(values)] for key, values in dict_rqa_files.items()}\n",
    "    #dictionary with mask names as keys and list of directories to .mat files depending on the side and subjects\n",
    "    return(dict_rqa_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_measure = {1:'DET', 2:'AverageDiagonal', 3:'MaxLine', 4:'entropy', 5:'laminarity', 6:'TT'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_matrices(all_directories, sides = ['Left', 'Right'], name_dict = 'rqaValuesMatrix', measures = [1,2,3,4,5,6], decode_measure = decode_measure):\n",
    "    #function taking the directory with files contatining data in.mat format with RQA measures valeus (all_directories),\n",
    "    #list with name left and right for the side where amygdala is located,\n",
    "    #name of matrix where rq`a values are located\n",
    "    #numbers of measures to take \n",
    "    #and dictionary for decoding numbers into RQA measures names\n",
    "    #and returning \n",
    "    dict_files = all_data(all_directories, sides) #dictionary with mask names as keys and list of directories to .mat files depending on the side and subjects\n",
    "    matrices_to_cluster = {(keys,decode_measure[measure]):[create_matrix(value_side, measure, name_dict) for value_side in values] for keys, values in dict_files.items() for measure in measures}\n",
    "    #dictionary with keys as names of mask and name of given measure and [list of matrices as values]\n",
    "    return(matrices_to_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_matrix(list_data, measure, name_in_dict):\n",
    "    #function taking the directory with files contatining data in.mat format with RQA measures valeus (list_data)\n",
    "    #measure number taken into account\n",
    "    #and name for the particular matrix with data\n",
    "    # and returnarray with data for a single subject\n",
    "    data_matrix2 = np.array([scipy.io.loadmat(data_matrix)[name_in_dict][:,measure] for data_matrix in list_data])\n",
    "    return(data_matrix2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_matrices(dict_matrices):\n",
    "    #function takes dictionary from create_matrices function as input and return dictionary with standardized and non-standardize values\n",
    "    nonstandard_matrices={(key, 'nonstandard'):value for key, value in dict_matrices.items()}\n",
    "    standard_matrices = {(key, 'standard'):[((side_value - np.mean(side_value))/np.std(side_value)) for side_value in value] for key,value in dict_matrices.items()}\n",
    "    standard_matrices.update(nonstandard_matrices)\n",
    "    return(standard_matrices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coordinates_function_dict(coordinatesDir, list_masks = ['Bach'], side_list = ['Left', 'Right']):\n",
    "    #function takes directory to files with mask of amygdala in MNI\n",
    "    #and return dict with matrices and coordinates\n",
    "    link_dict = {mask_name:[coordinatesDir + side + '/' + mask_name + 'Mask.nii.gz' for side in side_list] for mask_name in list_masks}\n",
    "    matrices_dict = {keys:[nib.load(side).get_data() for side in values] for keys, values in link_dict.items()}\n",
    "    coordinates_dict = {keys:[[(x_coor, y_coor, z_coor) for x_coor, x in enumerate(side) for y_coor, y in enumerate(x) for z_coor, z in enumerate(y) if z!=0] for side in values] for keys, values in matrices_dict.items()}\n",
    "    return([matrices_dict, coordinates_dict])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_clustering(result_dict, type_clustering = ['Hierarchy'], num_clustering = [2], linkage = 'ward', neighbors = True, coordinates_dict = coordinates_dict, num_repetitions = 1000):\n",
    "    #function takes dictionary with results and return dictionary after performing clustering\n",
    "    clustering_numerous = {keys:{(type_clust, num_clust):[[clustering_typeF(np.transpose(side_values), type_clust, num_clust, linkage, neighbors, coordinates_dict[keys[0][0]][num]) for i in range(num_repetitions)] for num, side_values in enumerate(values)] for type_clust in type_clustering for num_clust in num_clustering} for keys, values in result_dict.items()}\n",
    "    clustering_final_labels = {keys:{keys2:[np.round(np.mean(side, axis = 0)) for num, side in enumerate(arrays)] for keys2, arrays in values.items()} for keys, values in clustering_numerous.items()}\n",
    "    return(clustering_final_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering_typeF(dataset, typeClustering, numClust, linkage, neighbors, coordinates):\n",
    "    #function takes dataset to cluster and return return appropraite labels after clustering\n",
    "    if (typeClustering == 'KMeans'):\n",
    "        labels = KMeans_clustering(dataset, numClust)\n",
    "    elif (typeClustering == 'Hierarchy'):\n",
    "        labels = Ward_hierarchical_clustering(dataset, numClust, linkage, neighbors, coordinates)\n",
    "    elif (typeClustering == 'Spectral'):\n",
    "        labels = Spectral_clustering(dataset, numClust)\n",
    "    lengthLabels = [np.sum([1 for j in labels if i==j]) for i in range(1,numClust+1)]\n",
    "    lengthLabelsOrdered = sorted(lengthLabels, reverse=True)\n",
    "    if lengthLabelsOrdered[0]!=lengthLabelsOrdered[1]:\n",
    "        labels = changeLabels(labels, lengthLabels, lengthLabelsOrdered)\n",
    "    return(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def changeLabels(labels, lengthLabels, lengthLabelsOrdered):\n",
    "    #function ordered clusters - 1 - the biggest cluster, 2 - 2nd biggest cluster etc\n",
    "    original_labeling = {(num+1):i for num, i in enumerate(lengthLabels)}\n",
    "    original_labeling_reverse = {value:key for key,value in original_labeling.items()}\n",
    "    if (len(original_labeling.items())!=len(original_labeling_reverse.items())):\n",
    "        return(np.zeros(len(labels)))\n",
    "    appropriate_labeling = {(num+1):i for num, i in enumerate(lengthLabelsOrdered)} \n",
    "    \n",
    "    \n",
    "    for j in range(len(lengthLabels)):\n",
    "        if original_labeling[j+1] != appropriate_labeling[j+1]:\n",
    "             for key, value in appropriate_labeling.items():\n",
    "                if value == original_labeling[j+1]:\n",
    "                        labels[labels==original_labeling_reverse[value]] = key + len(lengthLabels)\n",
    "                        \n",
    "    for k in np.unique(labels):\n",
    "        if k > len(lengthLabels):\n",
    "            labels[labels==k]=k-len(lengthLabels)\n",
    "    return(labels)\n",
    "\n",
    "#performing KMeans Clustering\n",
    "\n",
    "def KMeans_clustering(i, n_clusters):    \n",
    "    RQA_matrix = np.array(i)\n",
    "    K_Means = KMeans(n_clusters).fit_predict(RQA_matrix) + 1\n",
    "    return(K_Means)\n",
    "\n",
    "#performing Agglomerative Clustering (Ward hierarchical clustering)\n",
    "\n",
    "def Ward_hierarchical_clustering(i, n_clusters, linkage, neighbors, coordinates):\n",
    "    RQA_matrix = np.array(i)       \n",
    "    if neighbors == True:\n",
    "        Ward = AgglomerativeClustering(n_clusters, linkage = linkage, ).fit_predict(RQA_matrix) + 1\n",
    "    else: \n",
    "        connectivity = rng(coordinates, radius =1, mode=\"connectivity\", metric = \"minkowski\", p=2, include_self=None)\n",
    "        Ward = AgglomerativeClustering(n_clusters, linkage = linkage, connectivity = connectivity).fit_predict(RQA_matrix) + 1\n",
    "    return(Ward)\n",
    "\n",
    "#performing Spectral Clustering\n",
    "\n",
    "def Spectral_clustering(i, n_clusters):\n",
    "    j = scipy.spatial.distance.pdist(i)\n",
    "    k = scipy.spatial.distance.squareform(j)\n",
    "    mean_dist = np.mean(k)\n",
    "    final_matrix = np.exp(-k/(2*(mean_dist**2)))\n",
    "    RQA_matrix = np.array(final_matrix)\n",
    "    Spectral = SpectralClustering(n_clusters, affinity = 'precomputed').fit(RQA_matrix) \n",
    "    Spectral = Spectral.labels_ + 1\n",
    "    return(Spectral)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_validations(clustering_labels_dict, coordinates_dict, matrices_dict, groundtruthDicts):\n",
    "    code_side = {0:'Left', 1:'Right'} #zakodowanie strony\n",
    "    #stability_indicator = {keys:{keys2:[np.mean([max({j:np.sum([1 for i in np.array(side)[:,voxel] if i==j])/num_repetitions for j in np.unique(np.array(side)[0,:])}.values()) for voxel in range(np.array(side).shape[1])]) for num, side in enumerate(arrays)] for keys2,arrays in values.items()} for keys, values in clustering_numerous.items()}\n",
    "    internal_validation_dict = {keys:{keys2:[internal_validation_test(side_vector, coordinates_dict[keys[0][0]][num]) for num, side_vector in enumerate(arrays)] for keys2, arrays in values.items()} for keys, values in clustering_labels_dict.items()} \n",
    "    external_validation_dict = {keys:{keys2:{keys3:[external_validation_measures(side_vector, keys[0][0], code_side[num], groundtruthDicts[keys3][code_side[num]],coordinates_dict, matrices_dict) for num, side_vector in enumerate(arrays)] for keys3, gtValues in groundTruthDicts.items()} for keys2, arrays in values.items()} for keys, values in clustering_labels_dict.items()}\n",
    "    external_validation_dict = {keys:{keys2:{keys3:{keys4:[dict_measures_side[keys4] for dict_measures_side in values3] for dict_measures_side in values3 for keys4, values4 in dict_measures_side.items()} for keys3, values3 in arrays.items()} for keys2, arrays in values.items()} for keys, values in external_validation_dict.items()}\n",
    "    external_validation_dict = {keys:{keys2:{keys3:{keys4:{keys5:[side_measures2[keys5] for side_measures2 in values4] for side_measures in values4 for keys5, values5 in side_measures.items()} for keys4, values4 in values3.items()} for keys3, values3 in arrays.items()} for keys2, arrays in values.items()} for keys, values in external_validation_dict.items()}\n",
    "    all_validation_results = {keys:{keys2:[values2, internal_validation_dict[keys][keys2], external_validation_dict[keys][keys2]] for keys2, values2 in values.items()} for keys, values in clustering_labels_dict.items()}\n",
    "    return(all_validation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groundTruthDicts = {'Julich':{'Left':'/Users/kbielski/Documents/Amygdala/Results Previous/AmygdalaReComputedData/ExternalValidationMasks/Julich/LeftClassificationJulich.nii.gz',\n",
    "                              'Right':'/Users/kbielski/Documents/Amygdala/Results Previous/AmygdalaReComputedData/ExternalValidationMasks/Julich/RightClassificationJulich.nii.gz'},\n",
    "                   'Bach':{'Left':'/Users/kbielski/Documents/Amygdala/Results Previous/AmygdalaReComputedData/ExternalValidationMasks/Bach/BachmaskResampledLeft.nii.gz',\n",
    "                           'Right':'/Users/kbielski/Documents/Amygdala/Results Previous/AmygdalaReComputedData/ExternalValidationMasks/Bach/BachmaskResampledRight.nii.gz'}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def internal_validation_test(clustering_vector, clustering_coordinates):\n",
    "    labels = np.unique(clustering_vector)\n",
    "    metrica = scipy.spatial.distance.squareform(scipy.spatial.distance.pdist(np.array(clustering_coordinates), 'euclidean'))\n",
    "    cluster_coordinates = np.array([np.array([list(clustering_coordinates[num]) for num,clus in enumerate(clustering_vector) if i==clus]) for i in labels])\n",
    "    pairs_of_clusters = [[num for num, label in enumerate(clustering_vector) if label == clust] for clust in labels]\n",
    "    distance_matrices = [scipy.spatial.distance.squareform(scipy.spatial.distance.pdist([clustering_coordinates[indices] for indices in clust])) for clust in pairs_of_clusters]\n",
    "    intra_cluster_distance = [np.sum(distance)/2 for distance in distance_matrices]\n",
    "    inter_cluster_distance = [np.sum([scipy.spatial.distance.euclidean(clustering_coordinates[indices_first], clustering_coordinates[indices_second]) for indices_first in indicesX for indices_second in indicesY]) \n",
    "                               for num, indicesX in enumerate(pairs_of_clusters) for indicesY in pairs_of_clusters[num+1:] if num!=(len(pairs_of_clusters)-1)]\n",
    "    all_inter_cluster_indices = [[(num, num1+num+1) for num1, indicesY in enumerate(pairs_of_clusters[num+1:]) if num!=(len(pairs_of_clusters)-1)] for num, indicesX in enumerate(pairs_of_clusters)]\n",
    "    labelki=[[(first_label, second_label) for pairs in all_inter_cluster_indices for (first_label, second_label) in pairs if first_label == (unique_label-1) or second_label == (unique_label-1)] for unique_label in labels]\n",
    "    ordered_labelki = [(num, num1+num+1) for num, indicesX in enumerate(pairs_of_clusters) for num1, indicesY in enumerate(pairs_of_clusters[num+1:])]\n",
    "    inter_cluster_distances_all = [sum([inter_cluster_distance[num] for (indX, indY) in indices for num,(indX_2, indY_2) in enumerate(ordered_labelki) if indX==indX_2 and indY == indY_2]) for indices in labelki]\n",
    "    intra_edges = [int(scipy.special.binom(len(coordinates),2)) for coordinates in cluster_coordinates]\n",
    "    inter_edges = [len(coordinates)*len(coordinatesNext) for num,coordinates in enumerate(cluster_coordinates) for coordinatesNext in cluster_coordinates[num+1:]]\n",
    "    Beta_CV = (np.sum(intra_cluster_distance)/np.sum(intra_edges))/(np.sum(inter_cluster_distance)/np.sum(inter_edges))\n",
    "    Normalized_Cut = np.sum([1/((intra_distance/inter_cluster_distances_all[num])+1) for num, intra_distance in enumerate(intra_cluster_distance)])/len(labels)\n",
    "    if (len(labels)==1):\n",
    "        silhouette_coefficient = 0\n",
    "    else:\n",
    "        silhouette_coefficient = silhouette_score(metrica, clustering_vector, metric='precomputed')\n",
    "    return([Beta_CV, Normalized_Cut, silhouette_coefficient])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def external_validation_measures(classifications, mask_name_main, side, ground_truths, coordinates_dict, matrices_dict):\n",
    "    [maskLabelsLimited, gtLabelsLimited, maskLabelsFull, gtLabelsFull] = mask_to_evaluate(classifications, mask_name_main, side, ground_truths, coordinates_dict, matrices_dict)\n",
    "    #purityLimited = {keys:[{one_label:np.sum([1 for x in gtLabelsLimited[keys][[num for num, label in enumerate(labels_limited) if label==num_clust] for num_clust in np.unique(labels_limited)] if x==one_label]) for one_label in np.unique(gtLabelsLimited[keys][[num for num, label in enumerate(labels_limited) if label==num_clust] for num_clust in np.unique(labels_limited)])} \n",
    "    lengthLimited = len(maskLabelsLimited) \n",
    "    lengthFull = len(maskLabelsFull)\n",
    "    names_list = ['Purity','FScore','MutualInfo','Jaccard','Fowlkes-Mallows']\n",
    "    purity = {'Limited': compute_purity(maskLabelsLimited, gtLabelsLimited, lengthLimited), 'Full': compute_purity(maskLabelsFull, gtLabelsFull, lengthFull)}\n",
    "    Fscore = {'Limited': compute_f_score(maskLabelsLimited, gtLabelsLimited), 'Full': compute_f_score(maskLabelsFull, gtLabelsFull)}\n",
    "    mutualInfo = {'Limited': compute_mutual_info(maskLabelsLimited, gtLabelsLimited), 'Full': compute_mutual_info(maskLabelsFull, gtLabelsFull)}\n",
    "    jaccard = {'Limited': compute_jaccard(maskLabelsLimited, gtLabelsLimited), 'Full': compute_jaccard(maskLabelsFull, gtLabelsFull)}\n",
    "    fowkles = {'Limited': compute_fawlkes(maskLabelsLimited, gtLabelsLimited), 'Full': compute_fawlkes(maskLabelsFull, gtLabelsFull)}\n",
    "    wholeExternalMeasures = {names_list[0]:purity,\n",
    "                             names_list[1]:Fscore,\n",
    "                             names_list[2]:mutualInfo,\n",
    "                             names_list[3]:jaccard,\n",
    "                             names_list[4]:fowkles} \n",
    "    return(wholeExternalMeasures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_fawlkes(maskLabels, gtLabels):\n",
    "    fawlkes = fowlkes_mallows_score(gtLabels, maskLabels)\n",
    "    return(fawlkes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_jaccard(maskLabels, gtLabels):\n",
    "    jaccard = jaccard_similarity_score(gtLabels, maskLabels)\n",
    "    return(jaccard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_f_score(maskLabels, gtLabels):\n",
    "    #print(len(maskLabels))\n",
    "    #print(len(gtLabels))\n",
    "    limited_Fscore=f1_score(gtLabels, maskLabels, average='micro')\n",
    "    return(limited_Fscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mutual_info(maskLabels, gtLabels):\n",
    "    mutual_info = adjusted_mutual_info_score(gtLabels, maskLabels) \n",
    "    return(mutual_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_purity(maskLabels, gtLabels, length):\n",
    "    indices_purity =[gtLabels[[num for num, label in enumerate(maskLabels) if label==num_clust]] for num_clust in np.unique(maskLabels)] \n",
    "    purity_dict_dict=[{unique_label:np.sum([1 for label in one_cluster if label==unique_label]) for unique_label in np.unique(one_cluster)} for num, one_cluster in enumerate(indices_purity)] \n",
    "    purity_dict = (1/length)*np.sum([sorted(list(dict_cluster.items()),key=lambda x: x[1], reverse = True)[0][1] for dict_cluster in purity_dict_dict]) \n",
    "    return(purity_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_to_evaluate(classifications, mask_name_main, side, ground_truths, coordinates_dict, matrices_dict):\n",
    "    side_decoder = {'Left':0, 'Right':1}\n",
    "    groundTruthFiles = open_ground_truth_file(ground_truths)\n",
    "    dict_coordinates = np.array(coordinates_dict[mask_name_main][side_decoder[side]])\n",
    "    [maskLabelsLimited, gtLabelsLimited] = limited_masks(groundTruthFiles, dict_coordinates, classifications, matrices_dict, mask_name_main, side_decoder[side])\n",
    "    [maskLabelsFull, gtLabelsFull] = full_masks(groundTruthFiles, dict_coordinates, classifications, matrices_dict, mask_name_main, side_decoder[side])\n",
    "    return([maskLabelsLimited, gtLabelsLimited, maskLabelsFull, gtLabelsFull])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limited_masks(groundTruthFiles, dict_coordinates, classifications, coordinatesDir, mask_name, side):\n",
    "    limited_mask_GT = [num for num, (x_1, y_1, z_1) in enumerate(groundTruthFiles[1]) for (x_2, y_2, z_2) in dict_coordinates if x_1 == x_2 and y_1 == y_2 and z_1 == z_2] \n",
    "    limited_mask_Original = [num for num, (x_1, y_1, z_1) in enumerate(dict_coordinates) for (x_2, y_2, z_2) in groundTruthFiles[1] if x_1 == x_2 and y_1 == y_2 and z_1 == z_2] \n",
    "    classification_limited_GT = [groundTruthFiles[0][limited_mask_GT],groundTruthFiles[1][limited_mask_GT]]\n",
    "    classification_limited_Original = [np.array(classifications)[limited_mask_Original], dict_coordinates[limited_mask_Original]]\n",
    "    maskLimited = labeling_limited(classification_limited_Original, matrices_dict, mask_name, side)\n",
    "    gtLimited = labeling_limited(classification_limited_GT, matrices_dict, mask_name, side)\n",
    "    return([maskLimited, gtLimited])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_masks(groundTruthFiles, dict_coordinates, classifications, matrices_dict, mask_name, side):\n",
    "    notInOriginal = [[x_1, y_1, z_1] for [x_1, y_1, z_1] in groundTruthFiles[1] if np.sum([1 for [x_2, y_2, z_2] in dict_coordinates if [x_1, y_1, z_1]!=[x_2, y_2, z_2]])==len(dict_coordinates)] \n",
    "    notInGT = [[x_1, y_1, z_1] for [x_1, y_1, z_1] in dict_coordinates if np.sum([1 for [x_2, y_2, z_2] in groundTruthFiles[1] if [x_1, y_1, z_1]!=[x_2, y_2, z_2]]) == len(groundTruthFiles[1])] \n",
    "    notInOriginalInd = [num for num, [x_1, y_1, z_1] in enumerate(groundTruthFiles[1]) if np.sum([1 for [x_2, y_2, z_2] in dict_coordinates if [x_1, y_1, z_1]!=[x_2, y_2, z_2]])==len(dict_coordinates)] \n",
    "    notInGTInd = [num for num, [x_1, y_1, z_1] in enumerate(dict_coordinates) if np.sum([1 for [x_2, y_2, z_2] in groundTruthFiles[1] if [x_1, y_1, z_1]!=[x_2, y_2, z_2]]) == len(groundTruthFiles[1])]\n",
    "    coordinatesFullOriginal = [list(j) for i in [dict_coordinates, notInOriginal] for j in i] \n",
    "    coordinatesFullGT = [list(j) for i in [groundTruthFiles[1], notInGT] for j in i] \n",
    "    classificationsOriginal = [j for i in [classifications,[max(classifications)+1]*len(notInOriginalInd)] for j in i] \n",
    "    classificationsGT = [j for i in [groundTruthFiles[0],[max(groundTruthFiles[0])+1]*len(notInGT)] for j in i] \n",
    "    labelsOriginal = labeling_full(classificationsOriginal, coordinatesFullOriginal, matrices_dict, mask_name, side)\n",
    "    labelsGT = labeling_full(classificationsGT, coordinatesFullGT, matrices_dict, mask_name, side)\n",
    "    return([labelsOriginal, labelsGT])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labeling_limited(classifications, matrices_dict, mask_name, side):\n",
    "    #print('Labeling:')\n",
    "    #print(classifications[0])\n",
    "    #print(classifications[1])\n",
    "    side_decoder = {'Left':0, 'Right':1}\n",
    "    data_sety=matrices_dict[mask_name][side]\n",
    "    empty_spaces = np.zeros(data_sety.shape)\n",
    "    for num,[x_1, y_1, z_1] in enumerate(classifications[1]):\n",
    "        empty_spaces[x_1][y_1][z_1]=classifications[0][num]\n",
    "    new_labeling = empty_spaces[empty_spaces!=0]\n",
    "    return(new_labeling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labeling_full(classifications, coordinates, matrices_dict, mask_name, side):\n",
    "    side_decoder = {'Left':0, 'Right':1}\n",
    "    data_sety=matrices_dict[mask_name][side]\n",
    "    empty_spaces = np.zeros(data_sety.shape) \n",
    "    for num,[x_1, y_1, z_1] in enumerate(coordinates):\n",
    "        empty_spaces[x_1][y_1][z_1]=classifications[num]\n",
    "    new_labeling =empty_spaces[empty_spaces!=0]\n",
    "    return(new_labeling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_coordinates(side, coordinatesDir, mask_name_main):\n",
    "    open_mask=nib.load(coordinatesDir + side +'/'+mask_name_main+'Mask.nii.gz')\n",
    "    load_data = open_mask.get_data()\n",
    "    coordinates = np.array([(x,y,z) for x, x_values in enumerate(load_data)\n",
    "                                    for y, y_values in enumerate(x_values)\n",
    "                                    for z, z_values in enumerate(y_values) if z_values!=0])\n",
    "    return(coordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_ground_truth_file(file_ground_truth):\n",
    "    loaded_file = nib.load(file_ground_truth)\n",
    "    data_file = loaded_file.get_data()\n",
    "    classificationGroundTruth = np.array([int(round(i)) for i in data_file[data_file!=0]])\n",
    "    coordinatesGroundTruth = np.array([(x,y,z) for x, value_x in enumerate(data_file) \n",
    "                                               for y, value_y in enumerate(value_x)\n",
    "                                               for z, value_z in enumerate(value_y) if value_z!=0])\n",
    "    return([classificationGroundTruth, coordinatesGroundTruth])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distances_test_original(testDataset, clusterTest):\n",
    "    intra_cluster_matrices = metrices_for_computing_distances(testDataset, clusterTest)\n",
    "    intra_cluster_distances = np.array([[[distance.euclidean(subset[:,ind_vect_one], subset[:,ind_vect_two]) for ind_vect_two in range(ind_vect_one+1, subset.shape[1]) if ind_vect_one!=subset.shape[1]-1] for ind_vect_one in range(subset.shape[1])] for subset in intra_cluster_matrices])\n",
    "    intra_cluster_sum_distances = [np.sum(two_array) for two_array in [np.sum(one_array) for one_array in intra_cluster_distances]]\n",
    "    intra_cluster_number_connections = [sum([len(two_array) for two_array in one_array]) for one_array in intra_cluster_distances]\n",
    "    intra_relative_sum_distances = [distances_intra/intra_cluster_number_connections[num] for num, distances_intra in enumerate(intra_cluster_sum_distances)]\n",
    "    return(intra_relative_sum_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrices_for_computing_distances(testDataset, clusterTest):\n",
    "    inter_cluster_indices = [[index for index, num_label in enumerate(clusterTest) if i == num_label] for i in np.unique(clusterTest)]\n",
    "    inter_cluster_matrices = [testDataset[:,indices] for indices in inter_cluster_indices]\n",
    "    return(inter_cluster_matrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distances_test_inter(testDataset, clusterTest):\n",
    "    inter_cluster_matrices = metrices_for_computing_distances(testDataset, clusterTest)\n",
    "    inter_cluster_distances = np.array([[[[distance.euclidean(matrix_one[:,vect_one], matrix_two[:,vect_two])] for vect_two in range(matrix_two.shape[1]) for vect_one in range(matrix_one.shape[1])] for matrix_two in inter_cluster_matrices[num+1:]] for num, matrix_one in enumerate(inter_cluster_matrices) if (num!=len(inter_cluster_matrices)-1)])\n",
    "    inter_cluster_sum_distances = [[np.sum(two_comparison) for two_comparison in one_comparison] for one_comparison in inter_cluster_distances]\n",
    "    inter_cluster_numbers = [[len(two_comparison) for two_comparison in one_comparison] for one_comparison in inter_cluster_distances]\n",
    "    inter_cluster_relative = [[second_distance/inter_cluster_numbers[num][num2] for num2, second_distance in enumerate(first_distance)] for num, first_distance in enumerate(inter_cluster_sum_distances)]\n",
    "    inter_cluster_relative = list(itertools.chain(*inter_cluster_relative))\n",
    "    return(inter_cluster_relative)\n",
    "    #inter_cluster_distances = [[[distance.euclidean(vect_one, vect_two) for vect_two in matrix_two for vect_one in matrix_one] for matrix_two in inter_cluster_matrices[num+1]] for num, matrix_one in enumerate(inter_cluster_matrices)]\n",
    "    #return(inter_cluster_distances)\n",
    "    #inter_cluster_distances = np.mean([distance.euclidean(i,j) for j in second_dataset for i in first_dataset for second_dataset in inter_cluster_matrices[num+1] for num, first_dataset in enumerate(inter_cluster_matrices) if (num!=len(inter_cluster_matrices)+1)])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_dict_validation(dict_validation):\n",
    "    modified_dict_validation={keys:[np.mean(values[0]), np.mean(values[1]), values[2], values[3]] for keys, values in dict_validation.items()}\n",
    "    return(modified_dict_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<list>.sort_values(by = [2], ascending = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing entropy between subdivisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_clustering(dataset, measures, algorithm):\n",
    "    [left_clustering, right_clustering] = dataset[measures][algorithm]\n",
    "    return([left_clustering, right_clustering])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_index_label(clustering, label):\n",
    "    indices = np.array([num for num, x in enumerate(clustering) if x==label])\n",
    "    return(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_raw_data(raw_dataset, measures):\n",
    "    [left_raw_data, right_raw_data] = raw_dataset[measures]\n",
    "    return([left_raw_data, right_raw_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_raw_data_cluster(raw_data, indices):\n",
    "    values_one_cluster = raw_data[:,indices]\n",
    "    return(values_one_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_unique_labels(clustering):\n",
    "    labels = np.unique(clustering)\n",
    "    return(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indices_of_parts(clustering, labels):\n",
    "    indices_all = [take_index_label(clustering, one_label) for one_label in labels]\n",
    "    return(indices_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_data_cluster(raw_data_one, indices_list):\n",
    "    values=[raw_data_one[:,indices] for indices in indices_list]\n",
    "    return(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_cluster_dataset(directory):\n",
    "    cluster_dataset=np.load(directory, allow_pickle=True)\n",
    "    return(cluster_dataset.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_raw_datset(directory_raw):\n",
    "    raw_dataset = np.load(directory_raw, allow_pickle=True)\n",
    "    return(raw_dataset.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_datasets(directory, directory_raw):\n",
    "    [cluster_dataset, raw_dataset] = [open_cluster_dataset(directory), open_raw_datset(directory_raw)]\n",
    "    return([cluster_dataset, raw_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_data(directory, directory_raw, measures, algorithm):\n",
    "    [cluster_dataset, raw_dataset] = open_datasets(directory, directory_raw)\n",
    "    [cluster_data, raw_data]=[take_clustering(cluster_dataset, measures, algorithm), take_raw_data(raw_dataset, measures)]\n",
    "    return([cluster_data, raw_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_data_side(directory, directory_raw, measures, algorithm, side):\n",
    "    [cluster_data, raw_data]=take_data(directory, directory_raw, measures, algorithm)\n",
    "    return(cluster_data[side], raw_data[side])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_one_side(directory, directory_raw, measures, algorithm, sides):\n",
    "    all_datasets =[take_data_side(directory, directory_raw, measures, algorithm, one_side) for one_side in sides]\n",
    "    return(all_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_labels(directory, directory_raw, measures, algorithm, sides):\n",
    "    unique_lables=[take_unique_labels(one_dataset[0]) for one_dataset in take_one_side(directory, directory_raw, measures, algorithm, sides)]\n",
    "    return(unique_lables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_indices(directory, directory_raw, measures, algorithm, sides):\n",
    "    indices=[indices_of_parts(one_dataset[0], take_labels(directory, directory_raw, measures, algorithm, sides)[num]) for num, one_dataset in enumerate(take_one_side(directory, directory_raw, measures, algorithm, sides))]\n",
    "    return(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_values(directory, directory_raw, measures, algorithm, sides):\n",
    "    values=[[one_dataset[1][:,num2labels] for num2labels in take_indices(directory, directory_raw, measures, algorithm, sides)[num]] for num, one_dataset in enumerate(take_one_side(directory, directory_raw, measures, algorithm, sides))]\n",
    "    return(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_means(directory, directory_raw, measures, algorithm, sides):\n",
    "    means_values = [[np.mean(cluster_values, axis = 0) for cluster_values in one_side_set] for one_side_set in take_values(directory, directory_raw, measures, algorithm, sides)]\n",
    "    return(means_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def levene_test_onetwo(directory, directory_raw, measures, algorithm, sides):\n",
    "    results_test = [scipy.stats.levene(one_data_means[0], one_data_means[1]) for one_data_means in compute_means(directory, directory_raw, measures, algorithm, sides)]\n",
    "    return(results_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_test(directory, directory_raw, measures, algorithm, sides):\n",
    "    results_test = [scipy.stats.ttest_ind(one_data_means[0], one_data_means[1], equal_var=False) for one_data_means in compute_means(directory, directory_raw, measures, algorithm, sides)]\n",
    "    return(results_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normality_test_onetwo(directory, directory_raw, measures, algorithm, sides):\n",
    "    results_test = [[scipy.stats.normaltest(normality) for normality in one_data_means] for one_data_means in compute_means(directory, directory_raw, measures, algorithm, sides)]\n",
    "    return(results_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normality_test_leftright(directory, directory_raw, measures, algorithm, sides):\n",
    "    leftright_test = [scipy.stats.normaltest(np.concatenate(one_data_means)) for one_data_means in compute_means(directory, directory_raw, measures, algorithm, sides)]\n",
    "    return(leftright_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def levene_test_leftright(directory, directory_raw, measures, algorithm, sides):\n",
    "    leftright_test = [np.concatenate(one_data_means) for one_data_means in compute_means(directory, directory_raw, measures, algorithm, sides)]\n",
    "    levene=scipy.stats.levene(leftright_test[0], leftright_test[1])\n",
    "    return(levene)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ttest_test_leftright(directory, directory_raw, measures, algorithm, sides):\n",
    "    leftright_test = [np.concatenate(one_data_means) for one_data_means in compute_means(directory, directory_raw, measures, algorithm, sides)]\n",
    "    ttest=scipy.stats.ttest_ind(leftright_test[0], leftright_test[1], equal_var=True)\n",
    "    return(ttest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_labels = ['Left VL', 'Right VL', 'Left DM', 'Right DM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean2018 = [np.mean(left2018_one), np.mean(right2018_one), np.mean(left2018_two), np.mean(right2018_two)]\n",
    "std2018 = [np.std(left2018_one)/np.sqrt(len(left2018_one)), np.std(right2018_one)/np.sqrt(len(right2018_one)), np.std(left2018_two)/np.sqrt(len(left2018_two)), np.std(right2018_two)/np.sqrt(len(right2018_two))]\n",
    "mean2019 = [np.mean(left2019_one), np.mean(right2019_one), np.mean(left2019_two), np.mean(right2019_two)]\n",
    "std2019 = [np.std(left2019_one)/np.sqrt(len(left2019_one)), np.std(right2019_one)/np.sqrt(len(right2019_one)), np.std(left2019_two)/np.sqrt(len(left2018_two)), np.std(right2019_two)/np.sqrt(len(right2019_two))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.axhline(y =0, color = 'black', linewidth = .5)\n",
    "ax.bar(x_labels, mean2019, yerr=std2019, align='center', alpha=0.5, ecolor='black', capsize=10)\n",
    "ax.set_ylabel('Value of Shanon\\'s entropy')\n",
    "ax.set_xlabel('Amygdala Subdivision')\n",
    "ax.plot([0,0, 2, 2], [0.33, 0.34, 0.34, 0.33], linewidth=1, color='k')\n",
    "ax.text(1,0.34,\"***\")\n",
    "#ax.plot([2, 2, 3, 3], [-0.66, -0.67, -0.67, -0.66], linewidth=1, color='k')\n",
    "#ax.text(2.5,-0.715,\"*\")\n",
    "ax.plot([1, 1, 3, 3], [-0.7, -0.71, -0.71, -0.7], linewidth=1, color='k')\n",
    "ax.text(2,-0.77,\"***\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
